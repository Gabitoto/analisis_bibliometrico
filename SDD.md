# SDD - Software Design Document
## Sistema de AnÃ¡lisis BibliomÃ©trico

---

## ğŸ“‹ InformaciÃ³n del Documento

**Proyecto:** AnÃ¡lisis BibliomÃ©trico - Computational Intelligence & Bio-inspired Computing  
**Autor:** Lucas Gabirondo  
**VersiÃ³n:** 1.1  
**Fecha:** 6 de Febrero, 2026  
**Estado:** DiseÃ±o TÃ©cnico - En RevisiÃ³n  
**Referencia:** PRD v1.1

---

## Tabla de Contenidos

1. [VisiÃ³n General de Arquitectura](#1-visiÃ³n-general-de-arquitectura)
2. [Capacidades del Sistema](#2-capacidades-del-sistema)
3. [DiseÃ±o de Componentes](#3-diseÃ±o-de-componentes)
4. [Modelo de Datos](#4-modelo-de-datos)
5. [Interfaces y APIs](#5-interfaces-y-apis)
6. [Patrones de DiseÃ±o](#6-patrones-de-diseÃ±o)
7. [Consideraciones de Seguridad](#7-consideraciones-de-seguridad)
8. [Estrategia de Testing](#8-estrategia-de-testing)
9. [Monitoreo y Observabilidad](#9-monitoreo-y-observabilidad)
10. [Diagramas TÃ©cnicos](#10-diagramas-tÃ©cnicos)

---

## 1. VisiÃ³n General de Arquitectura

### 1.1 Estilo ArquitectÃ³nico

El sistema sigue una **arquitectura de capas (Layered Architecture)** con separaciÃ³n clara de responsabilidades:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Presentation Layer (Power BI)          â”‚
â”‚  VisualizaciÃ³n, Dashboards, Reportes            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Business Logic Layer                   â”‚
â”‚  AnÃ¡lisis, Agregaciones, ValidaciÃ³n             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Access Layer (DAL)                  â”‚
â”‚  ORM, Repositories, Query Builders               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Layer (PostgreSQL)                  â”‚
â”‚  Persistencia, Transacciones, Constraints        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Integration Layer (ETL)                  â”‚
â”‚  Extractors, Transformers, Loaders               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    External Services (APIs AcadÃ©micas)           â”‚
â”‚  ArXiv API                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Principios de DiseÃ±o

- **Separation of Concerns**: Cada capa tiene responsabilidades bien definidas
- **DRY (Don't Repeat Yourself)**: Componentes reutilizables
- **SOLID Principles**: Aplicados en diseÃ±o de clases y mÃ³dulos
- **Fail-Fast**: ValidaciÃ³n temprana de datos
- **Idempotencia**: Operaciones ETL pueden re-ejecutarse sin efectos secundarios

### 1.3 Stack TecnolÃ³gico por Capa

| Capa | TecnologÃ­as |
|------|-------------|
| Presentation | Power BI Desktop/Service, DAX |
| Business Logic | Python 3.10+, pandas, numpy |
| Data Access | SQLAlchemy 2.0, psycopg2 |
| Data Storage | PostgreSQL 15+ (Supabase) |
| Integration | Python, requests, arxiv library |
| External | REST APIs (JSON over HTTPS) |

---

## 2. Capacidades del Sistema

### 2.1 Capacidad #1: Sistema de ExtracciÃ³n de Datos

**DescripciÃ³n**: Consumo automatizado de mÃºltiples APIs acadÃ©micas para obtener metadatos de publicaciones cientÃ­ficas.

**Requisitos Funcionales**:
- BÃºsqueda parametrizada por categorÃ­a, fecha, keywords
- PaginaciÃ³n para grandes volÃºmenes (>10,000 papers)
- Rate limiting inteligente (respeta lÃ­mites de APIs)
- Reintentos automÃ¡ticos con backoff exponencial
- Almacenamiento de respuestas raw en JSON

**Requisitos No Funcionales**:
- **Performance**: Extraer >5000 papers/hora
- **Reliability**: 99% de Ã©xito en requests (con retries)
- **Maintainability**: FÃ¡cil agregar nuevas fuentes de datos

**Componentes Involucrados**:
- `ArxivExtractor`
- `RateLimiter`
- `RetryManager`
- `CacheManager`

---

### 2.2 Capacidad #2: Sistema de TransformaciÃ³n de Datos

**DescripciÃ³n**: Pipeline de limpieza, normalizaciÃ³n y enriquecimiento de datos crudos.

**Requisitos Funcionales**:
- Limpieza de strings (encoding, caracteres especiales)
- NormalizaciÃ³n de fechas a ISO 8601
- ValidaciÃ³n de DOIs con regex
- DeduplicaciÃ³n por DOI y tÃ­tulo
- ExtracciÃ³n de keywords con NLP bÃ¡sico
- NormalizaciÃ³n de nombres de autores (ORCID)

**Requisitos No Funcionales**:
- **Data Quality**: >95% de registros vÃ¡lidos
- **Performance**: Procesar 10,000 papers en <5 minutos
- **Traceability**: Logs de cada transformaciÃ³n aplicada

**Componentes Involucrados**:
- `DataCleaner`
- `DateNormalizer`
- `DOIValidator`
- `DeduplicationEngine`
- `KeywordExtractor`
- `AuthorNormalizer`

---

### 2.3 Capacidad #3: Sistema de Persistencia

**DescripciÃ³n**: Almacenamiento robusto y escalable en base de datos relacional con integridad referencial.

**Requisitos Funcionales**:
- Carga incremental (upsert)
- Transacciones ACID
- Manejo de relaciones N:M
- AuditorÃ­a de cambios (triggers)
- Vistas materializadas para agregaciones

**Requisitos No Funcionales**:
- **Performance**: >1000 inserts/segundo (bulk)
- **Durability**: Backups automÃ¡ticos diarios
- **Scalability**: Soportar hasta 100,000 papers
- **Consistency**: Constraints de integridad referencial

**Componentes Involucrados**:
- `PostgresLoader`
- `UpsertManager`
- `TransactionManager`
- `ConnectionPool`
- `MigrationManager`

---

### 2.4 Capacidad #4: Sistema de VisualizaciÃ³n y BI

**DescripciÃ³n**: Dashboard interactivo para anÃ¡lisis exploratorio y generaciÃ³n de insights.

**Requisitos Funcionales**:
- GrÃ¡ficos de tendencias temporales
- Mapas geogrÃ¡ficos interactivos
- Scatter plots con correlaciones
- Filtros dinÃ¡micos (slicers)
- ExportaciÃ³n a PDF

**Requisitos No Funcionales**:
- **Performance**: Tiempo de carga <2 segundos
- **Usability**: Interfaz intuitiva sin capacitaciÃ³n
- **Responsiveness**: Interacciones <500ms
- **Accessibility**: Cumplir estÃ¡ndares WCAG 2.1

**Componentes Involucrados**:
- `KeywordTrendView`
- `GeographicDistributionMap`
- `CitationImpactAnalysis`
- `DAXMeasures`
- `SQLViews`

---

### 2.5 Capacidad #5: Sistema de AnÃ¡lisis Avanzado

**DescripciÃ³n**: MÃ³dulos de machine learning y anÃ¡lisis de redes para insights profundos (Fase Futura).

**Requisitos Funcionales**:
- AnÃ¡lisis de sentimiento en abstracts (NLP)
- Topic modeling con LDA
- AnÃ¡lisis de redes de colaboraciÃ³n
- PredicciÃ³n de tendencias con series temporales

**Requisitos No Funcionales**:
- **Accuracy**: >80% en clasificaciÃ³n de sentimiento
- **Performance**: Procesamiento batch de 10,000 abstracts <1 hora
- **Scalability**: Modelos reentrenables con nuevos datos
- **Interpretability**: Resultados explicables

**Componentes Involucrados**:
- `SentimentAnalyzer`
- `TopicModeler`
- `NetworkAnalyzer`
- `TrendPredictor`
- `MLPipeline`

---

## 3. DiseÃ±o de Componentes

### 3.1 Capa de IntegraciÃ³n (ETL Layer)

#### 3.1.1 BaseExtractor (Clase Abstracta)

**Responsabilidad**: Plantilla para todos los extractores de APIs.

**Atributos**:
- `api_url: str` - URL base de la API
- `rate_limiter: RateLimiter` - Control de tasa de requests
- `cache: CacheManager` - Cache de respuestas
- `retry_policy: RetryPolicy` - ConfiguraciÃ³n de reintentos

**MÃ©todos Principales**:
- `extract(query: Query) -> List[Dict]` - (Abstracto) Extraer datos
- `_make_request(endpoint: str, params: Dict) -> Response` - HTTP request con retries
- `_parse_response(response: Response) -> List[Dict]` - Parseo de JSON
- `_save_raw(data: List[Dict], filename: str) -> None` - Guardar respuesta raw

**Patrones Aplicados**:
- Template Method (mÃ©todo `extract`)
- Strategy (polÃ­tica de reintentos intercambiable)

---

#### 3.1.2 ArxivExtractor (Clase Concreta)

**Responsabilidad**: ExtracciÃ³n desde ArXiv API.

**Atributos EspecÃ­ficos**:
- `categories: List[str]` - CategorÃ­as a buscar (cs.AI, cs.NE, etc.)
- `max_results_per_query: int` - LÃ­mite de paginaciÃ³n

**MÃ©todos EspecÃ­ficos**:
- `extract_by_category(category: str, date_range: DateRange) -> List[Paper]`
- `extract_by_keywords(keywords: List[str]) -> List[Paper]`
- `_build_query_string(filters: Dict) -> str` - ConstrucciÃ³n de query ArXiv

**Dependencias**:
- `arxiv` library (oficial)
- `BaseExtractor`

---

#### 3.1.3 SemanticScholarExtractor (Clase Concreta)

**Responsabilidad**: Enriquecimiento con mÃ©tricas de citaciÃ³n.

**Atributos EspecÃ­ficos**:
- `api_key: str` - Credencial (variable de entorno)
- `fields: List[str]` - Campos a extraer (citations, authors.hIndex, etc.)

**MÃ©todos EspecÃ­ficos**:
- `extract_by_doi(doi: str) -> Paper`
- `extract_citations(paper_id: str) -> List[Citation]`
- `extract_author_metrics(author_id: str) -> AuthorMetrics`
- `_handle_rate_limit() -> None` - Espera si se alcanza lÃ­mite

**Consideraciones**:
- Rate limit: 100 req/min (sleeps inteligentes)
- Cache obligatorio para evitar requests duplicados

---

#### 3.1.4 DataCleaner (Servicio)

**Responsabilidad**: Limpieza y normalizaciÃ³n de datos crudos.

**MÃ©todos Principales**:
- `clean_text(text: str) -> str` - NormalizaciÃ³n de strings
- `normalize_date(date_str: str) -> datetime` - ConversiÃ³n a ISO 8601
- `validate_doi(doi: str) -> bool` - ValidaciÃ³n con regex
- `remove_duplicates(papers: List[Paper]) -> List[Paper]` - DeduplicaciÃ³n
- `extract_keywords(text: str, method: str) -> List[str]` - ExtracciÃ³n NLP

**Algoritmos de DeduplicaciÃ³n**:
1. Exact match por DOI (prioridad alta)
2. Fuzzy match por tÃ­tulo (similarity >90%)
3. Match por (autores + fecha + tÃ­tulo parcial)

**Validaciones de DOI**:
- Regex: `^10.\d{4,9}/[-._;()/:A-Z0-9]+$` (case insensitive)
- VerificaciÃ³n de checksum cuando aplique

---

#### 3.1.5 PostgresLoader (Servicio)

**Responsabilidad**: Carga eficiente a base de datos.

**MÃ©todos Principales**:
- `load_papers(papers: List[Paper]) -> LoadResult` - Carga con upsert
- `load_authors(authors: List[Author]) -> LoadResult` - Carga de autores
- `load_relationships(relationships: List[Relationship]) -> LoadResult` - Tablas N:M
- `bulk_insert(records: List[Dict], table: str) -> int` - InserciÃ³n masiva con COPY
- `create_checkpoint(batch_id: str) -> None` - Punto de recuperaciÃ³n

**Estrategia de Upsert**:
```sql
INSERT INTO papers (doi, title, abstract, ...)
VALUES (...)
ON CONFLICT (doi) 
DO UPDATE SET 
  title = EXCLUDED.title,
  updated_at = NOW()
RETURNING id;
```

**Manejo de Transacciones**:
- Batch size: 1000 registros por transacciÃ³n
- Rollback automÃ¡tico en caso de error
- Checkpoint cada 10,000 registros

---

### 3.2 Capa de Acceso a Datos (DAL)

#### 3.2.1 Repository Pattern

**Responsabilidad**: AbstracciÃ³n del acceso a base de datos.

**Interfaces Principales**:

```python
# PseudocÃ³digo - NO implementar todavÃ­a
class IPaperRepository:
    def get_by_id(paper_id: UUID) -> Paper
    def get_by_doi(doi: str) -> Optional[Paper]
    def find_by_category(category: str, limit: int) -> List[Paper]
    def search_by_keywords(keywords: List[str]) -> List[Paper]
    def save(paper: Paper) -> UUID
    def update(paper: Paper) -> bool
    def delete(paper_id: UUID) -> bool
```

**ImplementaciÃ³n con SQLAlchemy**:
- ORM para mapeo objeto-relacional
- Query builder para consultas complejas
- Session management con context managers
- Connection pooling automÃ¡tico

---

#### 3.2.2 Unit of Work Pattern

**Responsabilidad**: GestiÃ³n de transacciones y consistencia.

**Componentes**:
- `UnitOfWork` - Coordinador de transacciones
- `Session` - Contexto de base de datos (SQLAlchemy)
- `TransactionScope` - Context manager para ACID

**Ejemplo de Uso (PseudocÃ³digo)**:
```python
# NO implementar - solo diseÃ±o
with UnitOfWork() as uow:
    paper = uow.papers.get_by_doi(doi)
    paper.citation_count += 1
    uow.citations.add(new_citation)
    uow.commit()  # Atomic
```

---

### 3.3 Capa de LÃ³gica de Negocio

#### 3.3.1 ETLOrchestrator (Coordinador)

**Responsabilidad**: OrquestaciÃ³n del flujo ETL completo.

**Flujo de EjecuciÃ³n**:
1. **ExtracciÃ³n**: Llamar a extractores por categorÃ­a
2. **TransformaciÃ³n**: Aplicar pipeline de limpieza
3. **Carga**: Insertar en base de datos
4. **ValidaciÃ³n**: Verificar integridad post-carga
5. **Logging**: Registrar mÃ©tricas y errores

**MÃ©todos Principales**:
- `run_full_pipeline() -> PipelineResult` - EjecuciÃ³n completa
- `run_incremental_update() -> PipelineResult` - Solo nuevos datos
- `run_category(category: str) -> PipelineResult` - Por categorÃ­a especÃ­fica
- `rollback_batch(batch_id: str) -> bool` - Revertir carga fallida

**Estrategia de RecuperaciÃ³n**:
- Checkpoints cada N registros
- Estado guardado en tabla `etl_execution_log`
- Reinicio desde Ãºltimo checkpoint exitoso

---

#### 3.3.2 AnalyticsService (Agregaciones)

**Responsabilidad**: CÃ¡lculos de mÃ©tricas y agregaciones para BI.

**MÃ©todos Principales**:
- `calculate_keyword_growth(year_start: int, year_end: int) -> DataFrame`
- `get_top_cited_papers(category: str, top_n: int) -> List[Paper]`
- `calculate_author_collaboration_score(author_id: UUID) -> float`
- `get_geographic_distribution() -> Dict[str, int]`
- `calculate_correlation(field_x: str, field_y: str) -> float`

**Optimizaciones**:
- Uso de vistas materializadas en PostgreSQL
- Caching de resultados con TTL (Time To Live)
- Pre-cÃ¡lculo de mÃ©tricas frecuentes

---

### 3.4 Capa de PresentaciÃ³n

#### 3.4.1 PowerBIConnector (Conector)

**Responsabilidad**: Interfaz entre PostgreSQL y Power BI.

**ConexiÃ³n**:
- DirectQuery vs Import Mode (evaluaciÃ³n de trade-offs)
- Credenciales seguras (variables de entorno)
- Query folding para optimizaciÃ³n

**Vistas SQL Optimizadas**:
- `vw_keyword_trends` - AgregaciÃ³n temporal de keywords
- `vw_author_metrics` - MÃ©tricas de autores (h-index, papers count)
- `vw_citation_network` - Relaciones de citaciÃ³n
- `vw_geographic_distribution` - Papers por paÃ­s/instituciÃ³n

---

#### 3.4.2 DAX Measures (Medidas Calculadas)

**Ejemplos de Medidas Clave**:

```dax
// NO implementar - solo diseÃ±o
YoY_Growth = 
    VAR CurrentYearCount = [Total Papers]
    VAR PreviousYearCount = CALCULATE([Total Papers], DATEADD(Date[Year], -1, YEAR))
    RETURN DIVIDE(CurrentYearCount - PreviousYearCount, PreviousYearCount)

Average_Citations = AVERAGEX(Papers, Papers[citation_count])

Top_N_Keywords = TOPN(20, Keywords, [Keyword_Frequency], DESC)
```

---

### 3.5 Capa de AnÃ¡lisis Avanzado (Futuro)

#### 3.5.1 SentimentAnalyzer (NLP)

**Responsabilidad**: ClasificaciÃ³n de sentimiento en abstracts.

**Arquitectura**:
- **Modelo**: BERT pre-entrenado (HuggingFace)
- **Pipeline**: TokenizaciÃ³n â†’ Encoding â†’ Inferencia â†’ Postproceso
- **Batch Processing**: Procesar mÃºltiples abstracts simultÃ¡neamente

**Flujo de Datos**:
```
Abstract (texto) 
  â†’ Tokenizer (BERT) 
  â†’ Model.forward() 
  â†’ Softmax (probabilities) 
  â†’ ClasificaciÃ³n (positive/neutral/negative)
  â†’ Almacenar en BD (sentiment_score)
```

**Consideraciones**:
- GPU opcional (acelera 10-20x)
- Modelo en memoria (evitar recargas)
- ValidaciÃ³n con conjunto anotado manualmente

---

#### 3.5.2 TopicModeler (LDA)

**Responsabilidad**: Descubrimiento de tÃ³picos latentes.

**Algoritmo**: Latent Dirichlet Allocation (LDA)

**Pipeline de Entrenamiento**:
1. **Preprocesamiento**:
   - TokenizaciÃ³n (spaCy)
   - RemociÃ³n de stopwords
   - LemmatizaciÃ³n
   - ConstrucciÃ³n de diccionario y corpus
2. **Entrenamiento**:
   - Gensim LdaModel
   - OptimizaciÃ³n de nÃºmero de topics (coherence score)
   - ParÃ¡metros: alpha, beta (Dirichlet priors)
3. **EvaluaciÃ³n**:
   - Coherence score (C_v metric)
   - Perplexity
   - Topic interpretability (manual)
4. **VisualizaciÃ³n**:
   - pyLDAvis (HTML interactivo)

**Salidas**:
- Topics con distribuciones de palabras
- AsignaciÃ³n de papers a topics (probabilÃ­stica)
- VisualizaciÃ³n exploratoria

---

#### 3.5.3 NetworkAnalyzer (Grafos)

**Responsabilidad**: AnÃ¡lisis de redes de colaboraciÃ³n.

**RepresentaciÃ³n de Grafo**:
- **Nodos**: Autores
- **Edges**: Co-autorÃ­a (peso = nÃºmero de papers compartidos)
- **Atributos de nodos**: h-index, instituciÃ³n, paÃ­s
- **Atributos de edges**: peso, fechas de colaboraciÃ³n

**MÃ©tricas de Centralidad**:
- **Degree Centrality**: NÃºmero de colaboradores
- **Betweenness Centrality**: Autores "puente" entre comunidades
- **Closeness Centrality**: Proximidad al resto de la red
- **PageRank**: Influencia ponderada

**DetecciÃ³n de Comunidades**:
- Algoritmo Louvain (modularidad)
- IdentificaciÃ³n de clusters de investigaciÃ³n

**VisualizaciÃ³n**:
- Exportar a GEXF (Gephi)
- Layout: ForceAtlas2 o Fruchterman-Reingold

---

#### 3.5.4 TrendPredictor (Series Temporales)

**Responsabilidad**: PredicciÃ³n de tendencias futuras en keywords.

**Modelos a Evaluar**:
1. **ARIMA**: Auto-Regressive Integrated Moving Average
   - Bueno para tendencias lineales
   - Requiere stationarity
2. **Prophet** (Facebook):
   - Maneja estacionalidad automÃ¡ticamente
   - Robusto a missing data
3. **LSTM** (Deep Learning):
   - Captura patrones no-lineales complejos
   - Requiere mÃ¡s datos

**Pipeline de PredicciÃ³n**:
1. Agregar series temporales por keyword
2. Split train/test (temporal hold-out)
3. Entrenar modelos candidatos
4. ValidaciÃ³n con MAPE (Mean Absolute Percentage Error)
5. Seleccionar mejor modelo
6. PredicciÃ³n a 2 aÃ±os
7. Calcular intervalos de confianza (95%)

**Salidas**:
- Predicciones puntuales
- Intervalos de confianza
- Dashboard con grÃ¡ficos de tendencias futuras

---

## 4. Modelo de Datos

### 4.1 Esquema Entidad-RelaciÃ³n

**Entidades Principales**:

#### 4.1.1 Tabla: `papers`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK, DEFAULT uuid_generate_v4() | Identificador Ãºnico |
| `doi` | VARCHAR(255) | UNIQUE, NOT NULL | Digital Object Identifier |
| `title` | TEXT | NOT NULL | TÃ­tulo del paper |
| `abstract` | TEXT | NULLABLE | Resumen |
| `published_date` | DATE | NOT NULL | Fecha de publicaciÃ³n |
| `page_count` | INTEGER | NULLABLE | NÃºmero de pÃ¡ginas |
| `citation_count` | INTEGER | DEFAULT 0 | NÃºmero de citaciones |
| `sentiment_score` | DECIMAL(3,2) | NULLABLE | Score de sentimiento (-1 a 1) |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de inserciÃ³n |
| `updated_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de actualizaciÃ³n |

**Ãndices**:
- `idx_papers_doi` (UNIQUE)
- `idx_papers_published_date` (BTREE)
- `idx_papers_citation_count` (BTREE DESC)

---

#### 4.1.2 Tabla: `authors`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `name` | VARCHAR(255) | NOT NULL | Nombre completo |
| `orcid` | VARCHAR(19) | UNIQUE, NULLABLE | ORCID ID |
| `h_index` | INTEGER | NULLABLE | H-index del autor |
| `institution` | VARCHAR(255) | NULLABLE | InstituciÃ³n |
| `country` | VARCHAR(100) | NULLABLE | PaÃ­s |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de inserciÃ³n |

**Ãndices**:
- `idx_authors_orcid` (UNIQUE)
- `idx_authors_name` (BTREE)
- `idx_authors_country` (BTREE)

---

#### 4.1.3 Tabla: `keywords`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `keyword` | VARCHAR(100) | UNIQUE, NOT NULL | Palabra clave normalizada |
| `category` | VARCHAR(50) | NULLABLE | CategorÃ­a (cs.AI, cs.NE, etc.) |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de inserciÃ³n |

**Ãndices**:
- `idx_keywords_keyword` (UNIQUE)
- `idx_keywords_category` (BTREE)

---

#### 4.1.4 Tabla: `categories`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `code` | VARCHAR(20) | UNIQUE, NOT NULL | CÃ³digo de categorÃ­a (cs.AI) |
| `name` | VARCHAR(255) | NOT NULL | Nombre descriptivo |
| `description` | TEXT | NULLABLE | DescripciÃ³n de la categorÃ­a |

---

#### 4.1.5 Tabla: `citations`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `citing_paper_id` | UUID | FK â†’ papers(id) | Paper que cita |
| `cited_paper_id` | UUID | FK â†’ papers(id) | Paper citado |
| `citation_context` | TEXT | NULLABLE | Contexto de la citaciÃ³n |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de inserciÃ³n |

**Constraints**:
- `UNIQUE(citing_paper_id, cited_paper_id)` - Evitar duplicados
- `CHECK(citing_paper_id != cited_paper_id)` - Evitar auto-citaciÃ³n

---

### 4.2 Tablas de RelaciÃ³n (N:M)

#### 4.2.1 Tabla: `papers_authors`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `paper_id` | UUID | FK â†’ papers(id), PK | ID del paper |
| `author_id` | UUID | FK â†’ authors(id), PK | ID del autor |
| `author_order` | INTEGER | NOT NULL | Orden del autor (1=primero) |

**Constraints**:
- `PRIMARY KEY (paper_id, author_id)`

---

#### 4.2.2 Tabla: `papers_keywords`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `paper_id` | UUID | FK â†’ papers(id), PK | ID del paper |
| `keyword_id` | UUID | FK â†’ keywords(id), PK | ID del keyword |
| `relevance_score` | DECIMAL(3,2) | NULLABLE | Score de relevancia (0-1) |

**Constraints**:
- `PRIMARY KEY (paper_id, keyword_id)`

---

#### 4.2.3 Tabla: `papers_categories`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `paper_id` | UUID | FK â†’ papers(id), PK | ID del paper |
| `category_id` | UUID | FK â†’ categories(id), PK | ID de la categorÃ­a |

**Constraints**:
- `PRIMARY KEY (paper_id, category_id)`

---

### 4.3 Tablas de AuditorÃ­a y Control

#### 4.3.1 Tabla: `etl_execution_log`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `execution_id` | VARCHAR(50) | UNIQUE | ID de ejecuciÃ³n ETL |
| `start_time` | TIMESTAMP | NOT NULL | Inicio de ejecuciÃ³n |
| `end_time` | TIMESTAMP | NULLABLE | Fin de ejecuciÃ³n |
| `status` | VARCHAR(20) | NOT NULL | RUNNING/SUCCESS/FAILED |
| `records_extracted` | INTEGER | DEFAULT 0 | Papers extraÃ­dos |
| `records_loaded` | INTEGER | DEFAULT 0 | Papers cargados |
| `error_message` | TEXT | NULLABLE | Mensaje de error |
| `checkpoint_data` | JSONB | NULLABLE | Estado de checkpoint |

---

#### 4.3.2 Tabla: `api_cache`

| Columna | Tipo | Constraints | DescripciÃ³n |
|---------|------|-------------|-------------|
| `id` | UUID | PK | Identificador Ãºnico |
| `cache_key` | VARCHAR(255) | UNIQUE | Key del cache (hash de query) |
| `response_data` | JSONB | NOT NULL | Respuesta de API |
| `created_at` | TIMESTAMP | DEFAULT NOW() | Timestamp de cache |
| `expires_at` | TIMESTAMP | NOT NULL | ExpiraciÃ³n del cache |

**Ãndices**:
- `idx_api_cache_key` (UNIQUE)
- `idx_api_cache_expires_at` (BTREE)

---

### 4.4 Vistas Materializadas

#### 4.4.1 Vista: `vw_keyword_trends`

**PropÃ³sito**: AgregaciÃ³n de keywords por aÃ±o para anÃ¡lisis de tendencias.

**DefiniciÃ³n (PseudocÃ³digo SQL)**:
```sql
CREATE MATERIALIZED VIEW vw_keyword_trends AS
SELECT 
    k.keyword,
    k.category,
    EXTRACT(YEAR FROM p.published_date) AS year,
    COUNT(DISTINCT p.id) AS paper_count,
    AVG(p.citation_count) AS avg_citations
FROM keywords k
JOIN papers_keywords pk ON k.id = pk.keyword_id
JOIN papers p ON pk.paper_id = p.id
GROUP BY k.keyword, k.category, EXTRACT(YEAR FROM p.published_date)
ORDER BY year, paper_count DESC;
```

**Refresh**: Diario (scheduled job)

---

#### 4.4.2 Vista: `vw_author_metrics`

**PropÃ³sito**: MÃ©tricas agregadas de autores.

**DefiniciÃ³n (PseudocÃ³digo SQL)**:
```sql
CREATE MATERIALIZED VIEW vw_author_metrics AS
SELECT 
    a.id AS author_id,
    a.name,
    a.institution,
    a.country,
    COUNT(DISTINCT p.id) AS paper_count,
    SUM(p.citation_count) AS total_citations,
    AVG(p.citation_count) AS avg_citations_per_paper,
    MAX(p.published_date) AS last_publication_date
FROM authors a
JOIN papers_authors pa ON a.id = pa.author_id
JOIN papers p ON pa.paper_id = p.id
GROUP BY a.id, a.name, a.institution, a.country;
```

---

### 4.5 Triggers de AuditorÃ­a

#### 4.5.1 Trigger: `update_papers_timestamp`

**PropÃ³sito**: Actualizar automÃ¡ticamente `updated_at` en modificaciones.

**DefiniciÃ³n (PseudocÃ³digo SQL)**:
```sql
CREATE TRIGGER update_papers_timestamp
BEFORE UPDATE ON papers
FOR EACH ROW
EXECUTE FUNCTION update_timestamp_column();

-- Function
CREATE FUNCTION update_timestamp_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

---

### 4.6 NormalizaciÃ³n y Forma Normal

**Nivel de NormalizaciÃ³n**: 3NF (Tercera Forma Normal)

**JustificaciÃ³n**:
- Elimina redundancia de datos
- Facilita actualizaciones sin anomalÃ­as
- Mantiene integridad referencial
- Balance entre normalizaciÃ³n y performance

**Trade-offs**:
- âœ… Integridad de datos alta
- âœ… FÃ¡cil de mantener
- âš ï¸ Requires JOINs para queries complejas (mitigado con vistas materializadas)

---

## 5. Interfaces y APIs

### 5.1 Interfaces Externas (APIs AcadÃ©micas)

#### 5.1.1 ArXiv API

**Endpoint**: `http://export.arxiv.org/api/query`  
**Protocolo**: HTTP GET (REST-like)  
**Formato**: XML (Atom feed)

**ParÃ¡metros Clave**:
- `search_query`: Consulta de bÃºsqueda (cat:cs.AI AND ti:neural)
- `start`: Offset para paginaciÃ³n
- `max_results`: Resultados por pÃ¡gina (max 2000)
- `sortBy`: Criterio de orden (relevance, lastUpdatedDate, submittedDate)

**Rate Limiting**: 
- 1 request cada 3 segundos
- Max 100,000 resultados por query

**Ejemplo de Request**:
```
GET http://export.arxiv.org/api/query?search_query=cat:cs.AI&start=0&max_results=100&sortBy=submittedDate&sortOrder=descending
```

**Campos Relevantes en Response**:
- `entry/id` - ArXiv ID
- `entry/title` - TÃ­tulo
- `entry/summary` - Abstract
- `entry/published` - Fecha de publicaciÃ³n
- `entry/author/name` - Autores
- `entry/category` - CategorÃ­as

---

### 5.2 Interfaces Internas

#### 5.2.1 IExtractor (Interfaz)

**MÃ©todos**:
```python
# PseudocÃ³digo - NO implementar
class IExtractor(ABC):
    @abstractmethod
    def extract(self, query: Query) -> List[Dict]:
        """Extraer datos desde fuente externa"""
        pass
    
    @abstractmethod
    def validate_response(self, response: Any) -> bool:
        """Validar estructura de respuesta"""
        pass
    
    @abstractmethod
    def parse_to_domain_model(self, raw_data: Dict) -> Paper:
        """Convertir respuesta raw a modelo de dominio"""
        pass
```

---

#### 5.2.2 ITransformer (Interfaz)

**MÃ©todos**:
```python
# PseudocÃ³digo - NO implementar
class ITransformer(ABC):
    @abstractmethod
    def transform(self, data: List[Dict]) -> List[Dict]:
        """Aplicar transformaciones a datos"""
        pass
    
    @abstractmethod
    def validate(self, data: Dict) -> ValidationResult:
        """Validar calidad de datos"""
        pass
```

---

#### 5.2.3 ILoader (Interfaz)

**MÃ©todos**:
```python
# PseudocÃ³digo - NO implementar
class ILoader(ABC):
    @abstractmethod
    def load(self, data: List[Dict]) -> LoadResult:
        """Cargar datos a destino"""
        pass
    
    @abstractmethod
    def rollback(self, batch_id: str) -> bool:
        """Revertir carga fallida"""
        pass
```

---

### 5.3 Data Transfer Objects (DTOs)

#### 5.3.1 PaperDTO

**PropÃ³sito**: Transferencia de datos de papers entre capas.

**Estructura**:
```python
# PseudocÃ³digo - NO implementar
@dataclass
class PaperDTO:
    doi: str
    title: str
    abstract: Optional[str]
    published_date: date
    authors: List[str]
    keywords: List[str]
    categories: List[str]
    citation_count: int = 0
    page_count: Optional[int] = None
    external_ids: Dict[str, str] = field(default_factory=dict)
```

---

#### 5.3.2 QueryDTO

**PropÃ³sito**: Encapsular parÃ¡metros de bÃºsqueda.

**Estructura**:
```python
# PseudocÃ³digo - NO implementar
@dataclass
class QueryDTO:
    categories: List[str]
    keywords: List[str]
    date_range: Tuple[date, date]
    max_results: int = 1000
    sort_by: str = 'relevance'
```

---

## 6. Patrones de DiseÃ±o

### 6.1 Patrones Creacionales

#### 6.1.1 Factory Pattern

**Uso**: CreaciÃ³n de extractores segÃºn tipo de fuente.

**Componentes**:
- `ExtractorFactory`: Clase factory
- `ExtractorType`: Enum (ARXIV)

**Ventajas**:
- EncapsulaciÃ³n de lÃ³gica de creaciÃ³n
- FÃ¡cil agregar nuevos extractores
- ConfiguraciÃ³n centralizada

---

#### 6.1.2 Builder Pattern

**Uso**: ConstrucciÃ³n de queries complejas.

**Componentes**:
- `QueryBuilder`: Builder con fluent interface
- `Query`: Objeto inmutable resultante

**Ejemplo de Uso (PseudocÃ³digo)**:
```python
# NO implementar
query = (QueryBuilder()
    .with_categories(['cs.AI', 'cs.NE'])
    .with_keywords(['neural', 'genetic'])
    .with_date_range(date(2020, 1, 1), date(2024, 12, 31))
    .with_max_results(5000)
    .build())
```

---

### 6.2 Patrones Estructurales

#### 6.2.1 Adapter Pattern

**Uso**: Adaptar respuestas de diferentes APIs a formato comÃºn.

**Componentes**:
- `IAPIAdapter`: Interfaz comÃºn
- `ArxivAdapter`: ImplementaciÃ³n para ArXiv API

**Ventajas**:
- Interfaz uniforme para diferentes APIs
- Desacopla lÃ³gica de negocio de detalles de APIs
- Facilita testing (mock adapters)

---

#### 6.2.2 Decorator Pattern

**Uso**: Agregar funcionalidad a extractores (logging, caching, retry).

**Componentes**:
- `BaseExtractor`: Componente base
- `LoggingDecorator`: Agrega logging
- `CachingDecorator`: Agrega caching
- `RetryDecorator`: Agrega retry logic

**Ejemplo (PseudocÃ³digo)**:
```python
# NO implementar
extractor = ArxivExtractor()
extractor = LoggingDecorator(extractor)
extractor = CachingDecorator(extractor)
extractor = RetryDecorator(extractor, max_retries=3)
```

---

### 6.3 Patrones de Comportamiento

#### 6.3.1 Strategy Pattern

**Uso**: Intercambiar algoritmos de deduplicaciÃ³n.

**Componentes**:
- `IDeduplicationStrategy`: Interfaz
- `DOIDeduplication`: Por DOI exacto
- `FuzzyTitleDeduplication`: Por similitud de tÃ­tulo
- `CompositeDeduplication`: CombinaciÃ³n de estrategias

---

#### 6.3.2 Observer Pattern

**Uso**: Notificaciones de progreso de ETL.

**Componentes**:
- `ETLObservable`: Subject (proceso ETL)
- `IETLObserver`: Interfaz de observer
- `ConsoleProgressObserver`: Log a consola
- `DatabaseProgressObserver`: Log a BD

**Eventos**:
- `on_extraction_start()`
- `on_extraction_progress(processed: int, total: int)`
- `on_extraction_complete(result: ExtractionResult)`
- `on_error(error: Exception)`

---

#### 6.3.3 Chain of Responsibility

**Uso**: Pipeline de transformaciÃ³n de datos.

**Componentes**:
- `ITransformationHandler`: Interfaz con `handle()`
- `TextCleaningHandler`: Limpieza de texto
- `DateNormalizationHandler`: NormalizaciÃ³n de fechas
- `DOIValidationHandler`: ValidaciÃ³n de DOIs
- `DeduplicationHandler`: EliminaciÃ³n de duplicados

**Flujo**:
```
Data â†’ TextCleaning â†’ DateNormalization â†’ DOIValidation â†’ Deduplication â†’ Cleaned Data
```

---

#### 6.3.4 Template Method

**Uso**: Plantilla para proceso ETL.

**Componentes**:
- `ETLTemplate`: Clase abstracta con template method `run()`
- MÃ©todos abstractos: `extract()`, `transform()`, `load()`
- MÃ©todos hooks: `pre_extract()`, `post_load()`, `on_error()`

**Algoritmo**:
```python
# PseudocÃ³digo - NO implementar
def run(self):
    self.pre_extract()
    data = self.extract()
    self.post_extract(data)
    
    self.pre_transform()
    cleaned_data = self.transform(data)
    self.post_transform(cleaned_data)
    
    self.pre_load()
    result = self.load(cleaned_data)
    self.post_load(result)
    
    return result
```

---

## 7. Consideraciones de Seguridad

### 7.1 GestiÃ³n de Credenciales

#### 7.1.1 Variables de Entorno

**MÃ©todo**: Archivo `.env` con `python-dotenv`

**Buenas PrÃ¡cticas**:
- âŒ **NUNCA** commitear `.env` a Git (usar `.gitignore`)
- âœ… Proveer `.env.example` con template
- âœ… Validar presencia de variables al inicio
- âœ… Usar nombres descriptivos (prefijo `DB_`, `API_`, etc.)

**Ejemplo `.env`**:
```bash
# Database
DB_HOST=db.xxxxx.supabase.co
DB_PORT=5432
DB_NAME=postgres
DB_USER=postgres
DB_PASSWORD=your_secure_password_here
```

---

#### 7.1.2 ConexiÃ³n a Base de Datos

**Opciones**:
1. **Connection String Completa** (menos seguro):
   ```
   postgresql://user:password@host:port/dbname
   ```
2. **Componentes Separados** (recomendado):
   ```python
   host=os.getenv('DB_HOST')
   port=os.getenv('DB_PORT')
   ...
   ```

**Buenas PrÃ¡cticas**:
- Usar SSL/TLS para conexiones (sslmode='require')
- Credentials con privilegios mÃ­nimos necesarios
- Rotar passwords periÃ³dicamente
- Auditar accesos a base de datos

---

### 7.2 ValidaciÃ³n de Entrada

#### 7.2.1 ValidaciÃ³n de Datos Externos

**Riesgos**:
- InyecciÃ³n SQL (si se construyen queries dinÃ¡micas)
- XSS (si se muestran datos sin sanitizar)
- Data poisoning (datos maliciosos de APIs)

**Mitigaciones**:
- âœ… Usar ORM (SQLAlchemy) - previene SQL injection
- âœ… Validar tipos de datos con Pydantic
- âœ… Sanitizar strings antes de almacenar
- âœ… Limitar longitud de campos (evitar overflow)
- âœ… Validar formatos (DOI, ORCID con regex)

---

#### 7.2.2 Rate Limiting y Throttling

**PropÃ³sito**: Evitar abuso y respetar lÃ­mites de APIs.

**ImplementaciÃ³n**:
- Token bucket algorithm
- Sliding window log
- Sleep dinÃ¡mico segÃºn headers de respuesta (`X-RateLimit-Remaining`)

**Estrategia**:
```python
# PseudocÃ³digo - NO implementar
if rate_limiter.is_exceeded():
    sleep_time = rate_limiter.calculate_wait_time()
    logger.warning(f"Rate limit reached, sleeping {sleep_time}s")
    time.sleep(sleep_time)
```

---

### 7.3 Seguridad de Datos

#### 7.3.1 EncriptaciÃ³n

**En TrÃ¡nsito**:
- HTTPS para todas las APIs externas
- TLS 1.2+ para conexiones a PostgreSQL

**En Reposo**:
- Supabase ofrece encriptaciÃ³n at-rest por defecto
- Campos sensibles (si existen) con encriptaciÃ³n adicional

---

#### 7.3.2 Backups y RecuperaciÃ³n

**Estrategia de Backups**:
- **Frecuencia**: Diaria (automÃ¡tica en Supabase)
- **RetenciÃ³n**: 30 dÃ­as
- **Tipo**: Full backup + incremental

**Plan de RecuperaciÃ³n**:
1. Identificar punto de recuperaciÃ³n (RPO: Recovery Point Objective)
2. Restaurar desde backup mÃ¡s reciente
3. Aplicar logs transaccionales (si disponibles)
4. Verificar integridad de datos
5. Reiniciar servicios

**RTO (Recovery Time Objective)**: <4 horas

---

### 7.4 AuditorÃ­a y Logging

#### 7.4.1 Niveles de Logging

**Niveles**:
- `DEBUG`: InformaciÃ³n detallada (solo desarrollo)
- `INFO`: Eventos normales (inicio/fin de procesos)
- `WARNING`: Situaciones inesperadas pero recuperables
- `ERROR`: Errores que impiden operaciÃ³n especÃ­fica
- `CRITICAL`: Errores que impiden funcionamiento del sistema

**QuÃ© Loggear**:
- âœ… Inicio/fin de procesos ETL
- âœ… Errores de APIs (status code, mensaje)
- âœ… Validaciones fallidas (con detalles)
- âœ… MÃ©tricas de performance (tiempo, registros procesados)
- âŒ **NUNCA** loggear passwords o API keys

---

#### 7.4.2 Formato de Logs

**Formato**: JSON estructurado (facilita parsing y anÃ¡lisis)

**Campos Obligatorios**:
- `timestamp`: ISO 8601
- `level`: DEBUG/INFO/WARNING/ERROR/CRITICAL
- `message`: Mensaje descriptivo
- `module`: MÃ³dulo que genera el log
- `execution_id`: ID de ejecuciÃ³n ETL (correlaciÃ³n)

**Ejemplo**:
```json
{
  "timestamp": "2026-01-26T15:30:45.123Z",
  "level": "INFO",
  "message": "ArXiv extraction completed",
  "module": "extractors.arxiv_client",
  "execution_id": "etl-20260126-153045",
  "metadata": {
    "papers_extracted": 1523,
    "duration_seconds": 187.5
  }
}
```

---

## 8. Estrategia de Testing

### 8.1 PirÃ¡mide de Testing

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  E2E Tests  â”‚  10%
        â”‚   (Manual)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚Integration Testsâ”‚ 20%
       â”‚  (APIs, DB)     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Unit Tests     â”‚ 70%
      â”‚ (Componentes)    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8.2 Unit Tests

**Framework**: `pytest`

**Cobertura Objetivo**: >80%

**Componentes a Testear**:
- Extractores (con mocks de APIs)
- Transformadores (limpieza, validaciÃ³n)
- Validadores (DOI, fechas, emails)
- Utilities (helpers, formatters)

**Ejemplo de Test (PseudocÃ³digo)**:
```python
# NO implementar - solo diseÃ±o
def test_doi_validator_valid():
    validator = DOIValidator()
    assert validator.validate("10.1234/example.doi") == True

def test_doi_validator_invalid():
    validator = DOIValidator()
    assert validator.validate("invalid-doi") == False
    
def test_date_normalizer_iso_format():
    normalizer = DateNormalizer()
    result = normalizer.normalize("2024-01-15")
    assert result == date(2024, 1, 15)
```

**Buenas PrÃ¡cticas**:
- Tests independientes (no dependen de orden)
- Nombres descriptivos (test_<component>_<scenario>_<expected>)
- Usar fixtures para setup/teardown
- Mock de dependencias externas

---

### 8.3 Integration Tests

**Framework**: `pytest` + `pytest-docker`

**Alcance**:
- ConexiÃ³n a PostgreSQL (test database)
- Flujo completo de extractor â†’ transformer â†’ loader
- Vistas SQL y queries complejas

**Requisitos**:
- Base de datos de test (separada de producciÃ³n)
- Docker compose para servicios (PostgreSQL, etc.)
- Reset de estado entre tests

**Ejemplo (PseudocÃ³digo)**:
```python
# NO implementar - solo diseÃ±o
def test_etl_pipeline_integration(test_db):
    # Arrange
    extractor = ArxivExtractor()
    transformer = DataCleaner()
    loader = PostgresLoader(test_db)
    
    # Act
    raw_data = extractor.extract(sample_query)
    cleaned_data = transformer.transform(raw_data)
    result = loader.load(cleaned_data)
    
    # Assert
    assert result.success == True
    assert result.loaded_count == len(cleaned_data)
    
    # Verify in DB
    papers = test_db.query("SELECT * FROM papers")
    assert len(papers) == result.loaded_count
```

---

### 8.4 End-to-End Tests

**Alcance**: Flujo completo de usuario (manual o automatizado)

**Escenarios**:
1. **Escenario 1**: ExtracciÃ³n de papers â†’ VisualizaciÃ³n en Power BI
   - Ejecutar ETL completo
   - Verificar que datos aparecen en dashboard
   - Validar mÃ©tricas calculadas

2. **Escenario 2**: ActualizaciÃ³n incremental
   - Ejecutar ETL con nuevos datos
   - Verificar que solo se agregan nuevos papers
   - Validar que no hay duplicados

**AutomatizaciÃ³n**: Considerar Selenium/Playwright para Power BI (difÃ­cil, prioridad baja)

---

### 8.5 Test Data Management

#### 8.5.1 Fixtures

**PropÃ³sito**: Datos de prueba reutilizables.

**UbicaciÃ³n**: `tests/fixtures/`

**Tipos**:
- `sample_arxiv_response.json` - Respuesta de ArXiv API
- `sample_papers.json` - Papers de ejemplo
- `sample_authors.json` - Autores de ejemplo

---

#### 8.5.2 Database Seeding

**PropÃ³sito**: Poblar DB de test con datos realistas.

**Estrategia**:
- Script de seeding (`tests/seed_test_db.py`)
- Ejecutar antes de integration tests
- Incluir casos edge (duplicados, nulls, etc.)

---

### 8.6 Mocking Strategy

**Principio**: Mockear dependencias externas, no lÃ³gica interna.

**QuÃ© Mockear**:
- âœ… Llamadas a APIs externas (requests)
- âœ… Conexiones a base de datos (en unit tests)
- âœ… Filesystem (si se usa)
- âŒ LÃ³gica de negocio (no mockear lo que estÃ¡s testeando)

**Herramientas**:
- `unittest.mock` (built-in)
- `pytest-mock` (plugin de pytest)
- `responses` (para mockear requests HTTP)

**Ejemplo (PseudocÃ³digo)**:
```python
# NO implementar - solo diseÃ±o
@patch('requests.get')
def test_arxiv_extractor_calls_api(mock_get):
    # Arrange
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.text = load_fixture('sample_arxiv_response.xml')
    mock_get.return_value = mock_response
    
    extractor = ArxivExtractor()
    
    # Act
    result = extractor.extract(Query(categories=['cs.AI']))
    
    # Assert
    mock_get.assert_called_once()
    assert len(result) > 0
```

---

## 9. Monitoreo y Observabilidad

### 9.1 MÃ©tricas Clave (KPIs)

#### 9.1.1 MÃ©tricas de ETL

| MÃ©trica | DescripciÃ³n | Objetivo |
|---------|-------------|----------|
| **Extraction Rate** | Papers extraÃ­dos por hora | >5000/hora |
| **Success Rate** | % de requests exitosos | >99% |
| **Data Quality Score** | % de registros vÃ¡lidos | >95% |
| **Load Throughput** | Inserts por segundo | >1000/seg |
| **Pipeline Duration** | Tiempo total de ETL | <2 horas (10K papers) |
| **Duplicate Rate** | % de duplicados detectados | Monitoreado |

---

#### 9.1.2 MÃ©tricas de Sistema

| MÃ©trica | DescripciÃ³n | Umbral de Alerta |
|---------|-------------|------------------|
| **Database Connections** | Conexiones activas | >80% del pool |
| **Query Performance** | Tiempo de queries lentas | >500ms |
| **Disk Usage** | Espacio usado en disco | >80% |
| **API Rate Limit Usage** | % de rate limit usado | >90% |

---

### 9.2 Logging Centralizado

#### 9.2.1 Estrategia

**Destinos de Logs**:
1. **Consola** (desarrollo): Logs estructurados con colores
2. **Archivo** (producciÃ³n): RotaciÃ³n diaria, retenciÃ³n 30 dÃ­as
3. **Base de Datos** (producciÃ³n): Eventos crÃ­ticos en tabla `etl_execution_log`

**RotaciÃ³n de Archivos**:
- RotaciÃ³n diaria a medianoche
- Formato: `etl-YYYY-MM-DD.log`
- CompresiÃ³n de logs >7 dÃ­as
- EliminaciÃ³n de logs >30 dÃ­as

---

#### 9.2.2 Estructura de Logs

**Logger Hierarchy**:
```
root
â”œâ”€â”€ extractors
â”‚   â””â”€â”€ arxiv
â”œâ”€â”€ transformers
â”‚   â”œâ”€â”€ cleaner
â”‚   â””â”€â”€ validator
â”œâ”€â”€ loaders
â”‚   â””â”€â”€ postgres
â””â”€â”€ utils
    â”œâ”€â”€ cache
    â””â”€â”€ retry
```

**ConfiguraciÃ³n por Ambiente**:
- **Development**: DEBUG level, consola con colores
- **Production**: INFO level, archivo + BD

---

### 9.3 Alertas y Notificaciones

#### 9.3.1 Condiciones de Alerta

| Alerta | CondiciÃ³n | Severidad | AcciÃ³n |
|--------|-----------|-----------|--------|
| **ETL Failure** | Pipeline falla completamente | CRITICAL | Email inmediato |
| **Low Success Rate** | <95% de requests exitosos | HIGH | Investigar APIs |
| **High Error Rate** | >5% de errores de validaciÃ³n | MEDIUM | Revisar datos |
| **Slow Query** | Query >2 segundos | LOW | Optimizar query |
| **Disk Space** | >90% usado | HIGH | Limpiar/expandir |

---

#### 9.3.2 Canales de NotificaciÃ³n

**Opciones** (segÃºn implementaciÃ³n):
- Email (SMTP)
- Slack/Discord webhooks
- SMS (Twilio) para crÃ­ticos
- Dashboard de monitoreo (Grafana, si se implementa)

---

### 9.4 Health Checks

#### 9.4.1 Database Health Check

**Verificaciones**:
- Conectividad a PostgreSQL
- Latencia de queries (<100ms para query simple)
- NÃºmero de conexiones activas
- Espacio disponible en disco

**Frecuencia**: Cada 5 minutos

---

#### 9.4.2 ETL Process Health Check

**Verificaciones**:
- Ãšltima ejecuciÃ³n exitosa (<24 horas)
- Progreso actual (si estÃ¡ corriendo)
- Tasa de errores en ventana de 1 hora

**Frecuencia**: Cada 15 minutos

---

### 9.5 Performance Monitoring

#### 9.5.1 Profiling

**Herramientas**:
- `cProfile` (built-in Python)
- `py-spy` (sampling profiler)
- `memory_profiler` (uso de memoria)

**CuÃ¡ndo Usar**:
- Durante desarrollo para identificar bottlenecks
- Si se observa degradaciÃ³n de performance en producciÃ³n
- Antes de optimizaciones mayores (establecer baseline)

---

#### 9.5.2 Query Performance

**Herramientas**:
- PostgreSQL `EXPLAIN ANALYZE`
- `pg_stat_statements` extension
- Slow query log

**Optimizaciones**:
- Crear Ã­ndices en columnas frecuentemente filtradas
- Usar vistas materializadas para agregaciones
- Particionar tablas grandes (si >1M rows)

---

## 10. Diagramas TÃ©cnicos

### 10.1 Diagrama de Arquitectura de Alto Nivel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Power BI Desktop                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Keyword    â”‚  â”‚ Geographic  â”‚  â”‚  Citation    â”‚             â”‚
â”‚  â”‚  Trends     â”‚  â”‚    Map      â”‚  â”‚  Analysis    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ DirectQuery / Import
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PostgreSQL (Supabase)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ papers â”‚ â”‚authors â”‚ â”‚ keywords â”‚ â”‚ categoriesâ”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ Materialized Viewsâ”‚  â”‚ Audit Tables   â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†‘ Bulk Insert (COPY)
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ETL Orchestrator                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Extract       â†’    Transform      â†’      Load            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚Extractorsâ”‚   â†’   â”‚Cleaners â”‚    â†’    â”‚PostgreSQLâ”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ Loader   â”‚    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚  â”‚ Cache   â”‚        â”‚Validatorsâ”‚                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“ HTTP Requests (REST)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     External APIs                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚  ArXiv API   â”‚                                                 â”‚
â”‚  â”‚  (XML/Atom)  â”‚                                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 10.2 Diagrama de Flujo ETL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   START     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Configuration   â”‚ (DB credentials, API keys, query params)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize Services  â”‚ (Extractors, Transformers, Loaders)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For Each Category    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
       â†“                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  EXTRACT             â”‚                   â”‚
â”‚  â””â”€ ArXiv API        â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
       â†“                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚ Cache Raw Response   â”‚ (JSON files)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
       â†“                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  TRANSFORM           â”‚                   â”‚
â”‚  â”œâ”€ Clean Text       â”‚                   â”‚
â”‚  â”œâ”€ Normalize Dates  â”‚                   â”‚
â”‚  â”œâ”€ Validate DOIs    â”‚                   â”‚
â”‚  â””â”€ Deduplicate      â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
       â†“                                    â”‚
    [Valid?]                                â”‚
       â”œâ”€ No â†’ Log Error â†’ Continue â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ Yes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOAD                â”‚
â”‚  â”œâ”€ Upsert Papers    â”‚
â”‚  â”œâ”€ Insert Authors   â”‚
â”‚  â”œâ”€ Insert Keywords  â”‚
â”‚  â””â”€ Link Relationshipsâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Checkpoint    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    [More Categories?]
       â”œâ”€ Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ No
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Refresh Materialized â”‚
â”‚ Views                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Log Final Metrics    â”‚ (duration, counts, errors)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Send Notifications   â”‚ (if errors or completion)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     END     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 10.3 Diagrama de Clases (Simplificado)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   <<interface>>         â”‚
â”‚    IExtractor           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + extract(query)        â”‚
â”‚ + validate_response()   â”‚
â”‚ + parse_to_model()      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ implements
            â”‚
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArxivExtractor   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚- categories        â”‚
â”‚+ extract()         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DataCleaner           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + clean_text()          â”‚
â”‚ + normalize_date()      â”‚
â”‚ + validate_doi()        â”‚
â”‚ + remove_duplicates()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgresLoader        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - connection_pool       â”‚
â”‚ + load_papers()         â”‚
â”‚ + load_authors()        â”‚
â”‚ + bulk_insert()         â”‚
â”‚ + create_checkpoint()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ETLOrchestrator       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - extractors: List      â”‚
â”‚ - transformer           â”‚
â”‚ - loader                â”‚
â”‚ + run_full_pipeline()   â”‚
â”‚ + run_incremental()     â”‚
â”‚ + rollback_batch()      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 10.4 Diagrama de Secuencia: Proceso de ExtracciÃ³n

```
User       ETLOrchestrator   ArxivExtractor   RateLimiter   ArXivAPI   CacheManager
 â”‚               â”‚                 â”‚               â”‚            â”‚            â”‚
 â”‚â”€run_pipeline()â”€>â”‚                â”‚               â”‚            â”‚            â”‚
 â”‚               â”‚â”€â”€extract()â”€â”€â”€â”€â”€>â”‚               â”‚            â”‚            â”‚
 â”‚               â”‚                 â”‚â”€check_limit()>â”‚            â”‚            â”‚
 â”‚               â”‚                 â”‚<â”€okâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚            â”‚            â”‚
 â”‚               â”‚                 â”‚â”€check_cache()â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚            â”‚
 â”‚               â”‚                 â”‚<â”€cache_missâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚            â”‚
 â”‚               â”‚                 â”‚â”€â”€â”€GET /api/query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   â”‚
 â”‚               â”‚                 â”‚<â”€â”€200 OK (XML)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
 â”‚               â”‚                 â”‚â”€save_to_cache()â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   â”‚
 â”‚               â”‚                 â”‚â”€parse_xml()â”€>â”‚            â”‚            â”‚
 â”‚               â”‚<â”€List[Paper]â”€â”€â”€â”€â”‚               â”‚            â”‚            â”‚
 â”‚<â”€â”€successâ”€â”€â”€â”€â”€â”‚                 â”‚               â”‚            â”‚            â”‚
```

---

### 10.5 Diagrama Entidad-RelaciÃ³n (Simplificado)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   papers     â”‚         â”‚  papers_authors  â”‚         â”‚   authors    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ paper_id (FK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ id (PK)      â”‚
â”‚ doi (UNIQUE) â”‚         â”‚ author_id (FK)   â”‚         â”‚ name         â”‚
â”‚ title        â”‚         â”‚ author_order     â”‚         â”‚ orcid        â”‚
â”‚ abstract     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ h_index      â”‚
â”‚ published_   â”‚                                      â”‚ institution  â”‚
â”‚  _date       â”‚                                      â”‚ country      â”‚
â”‚ citation_    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  _count      â”‚         â”‚ papers_keywords  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚                 â”‚ paper_id (FK)    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ keyword_id (FK)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    keywords      â”‚
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚ id (PK)          â”‚
                         â”‚ keyword (UNIQUE) â”‚
                         â”‚ category         â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  citations   â”‚         â”‚   categories     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)      â”‚         â”‚ id (PK)          â”‚
â”‚ citing_      â”‚         â”‚ code (UNIQUE)    â”‚
â”‚  _paper_id   â”‚â”€â”€â”      â”‚ name             â”‚
â”‚ cited_       â”‚  â”‚      â”‚ description      â”‚
â”‚  _paper_id   â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      â”‚papers_categories â”‚
                  â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â””â”€â”€â”€â”€â”€>â”‚ paper_id (FK)    â”‚
                         â”‚ category_id (FK) â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Plan de ImplementaciÃ³n (High-Level)

### 11.1 Fase 1: Fundamentos

**Componentes**:
- ConfiguraciÃ³n del proyecto (estructura de carpetas)
- Setup de base de datos (DDL scripts)
- Modelos de dominio (dataclasses/Pydantic)
- Utilities bÃ¡sicas (logger, config manager)

**DuraciÃ³n Estimada**: NO especificar

---

### 11.2 Fase 2: ETL Core

**Componentes**:
- ImplementaciÃ³n de extractores
- Pipeline de transformaciÃ³n
- Loader a PostgreSQL
- Orchestrator bÃ¡sico

**DuraciÃ³n Estimada**: NO especificar

---

### 11.3 Fase 3: VisualizaciÃ³n

**Componentes**:
- Vistas SQL optimizadas
- ConexiÃ³n Power BI
- Dashboards interactivos
- Medidas DAX

**DuraciÃ³n Estimada**: NO especificar

---

### 11.4 Fase 4: AnÃ¡lisis Avanzado (Futuro)

**Componentes**:
- MÃ³dulos NLP
- AnÃ¡lisis de redes
- Modelos predictivos
- Visualizaciones avanzadas

**DuraciÃ³n Estimada**: NO especificar

---

## 12. Dependencias y Versiones

### 12.1 Dependencias Core

```txt
python>=3.10
pandas>=2.0.0
numpy>=1.24.0
psycopg2-binary>=2.9.0
sqlalchemy>=2.0.0
requests>=2.31.0
arxiv>=2.0.0
python-dotenv>=1.0.0
```

### 12.2 Dependencias de Testing

```txt
pytest>=7.4.0
pytest-mock>=3.11.0
pytest-cov>=4.1.0
pytest-docker>=2.0.0
responses>=0.23.0
```

### 12.3 Dependencias Futuras (NLP)

```txt
transformers>=4.30.0
gensim>=4.3.0
spacy>=3.5.0
networkx>=3.1
scikit-learn>=1.3.0
```

---

## 13. Glosario TÃ©cnico

- **ACID**: Atomicity, Consistency, Isolation, Durability
- **DAL**: Data Access Layer
- **DTO**: Data Transfer Object
- **ETL**: Extract, Transform, Load
- **ORM**: Object-Relational Mapping
- **SOLID**: Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, Dependency Inversion
- **3NF**: Tercera Forma Normal (normalizaciÃ³n de BD)
- **Idempotencia**: OperaciÃ³n que produce el mismo resultado si se ejecuta mÃºltiples veces
- **Backoff Exponencial**: Estrategia de retry con esperas crecientes (1s, 2s, 4s, 8s...)
- **Upsert**: INSERT + UPDATE (insertar si no existe, actualizar si existe)
- **Rate Limiting**: Control de tasa de requests para no exceder lÃ­mites
- **Materialized View**: Vista SQL pre-calculada y almacenada fÃ­sicamente

---

## 14. Referencias

### 14.1 DocumentaciÃ³n de APIs

- ArXiv API: https://arxiv.org/help/api/

### 14.2 DocumentaciÃ³n TÃ©cnica

- SQLAlchemy 2.0: https://docs.sqlalchemy.org/
- PostgreSQL 15: https://www.postgresql.org/docs/15/
- Power BI: https://learn.microsoft.com/power-bi/
- Pytest: https://docs.pytest.org/

### 14.3 Patrones de DiseÃ±o

- Gang of Four Design Patterns
- Martin Fowler - Patterns of Enterprise Application Architecture
- Domain-Driven Design (DDD) - Eric Evans

---

## 15. Control de Cambios

| VersiÃ³n | Fecha | Autor | Cambios |
|---------|-------|-------|---------|
| 1.0 | 2026-01-26 | Lucas Gabirondo | VersiÃ³n inicial del SDD |

---

## 16. Aprobaciones

| Rol | Nombre | Fecha | Firma |
|-----|--------|-------|-------|
| Tech Lead | - | - | - |
| Architect | - | - | - |
| Product Owner | Lucas Gabirondo | 2026-01-26 | âœ“ |

---

**Ãšltima actualizaciÃ³n:** 26 de Enero, 2026  
**PrÃ³xima revisiÃ³n:** 15 de Febrero, 2026  
**Estado:** DRAFT - Pendiente de RevisiÃ³n TÃ©cnica
