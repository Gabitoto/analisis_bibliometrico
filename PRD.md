# PRD - An√°lisis Bibliom√©trico: Inteligencia Computacional y Computaci√≥n Bio-inspirada

## üìã Informaci√≥n del Documento

**Proyecto:** An√°lisis Bibliom√©trico - Computational Intelligence & Bio-inspired Computing  
**Autor:** Lucas Gabirondo  
**Versi√≥n:** 1.1  
**Fecha:** 6 de Febrero, 2026  
**Estado:** En Desarrollo (Fase de Construcci√≥n de Base de Datos)

---

## 1. Resumen Ejecutivo

### 1.1 Visi√≥n del Producto
Desarrollar una base de datos estructurada de publicaciones acad√©micas del campo de la Inteligencia Computacional y Computaci√≥n Bio-inspirada, extrayendo metadatos desde ArXiv para habilitar futuros an√°lisis bibliom√©tricos y visualizaciones.

### 1.2 Objetivos del Negocio
- Automatizar la extracci√≥n y an√°lisis de publicaciones acad√©micas desde ArXiv
- Proporcionar insights sobre tendencias emergentes en investigaci√≥n
- Identificar patrones de colaboraci√≥n entre investigadores
- Facilitar la toma de decisiones para investigadores y acad√©micos

### 1.3 M√©tricas de √âxito
- Ingesta exitosa de >5,000 publicaciones acad√©micas desde ArXiv
- Base de datos normalizada (3NF) con integridad referencial
- Pipeline ETL funcional y documentado
- Datos limpios y listos para an√°lisis con <1% de errores de validaci√≥n

---

## 2. Stack Tecnol√≥gico

| Componente | Tecnolog√≠a | Justificaci√≥n |
|------------|-----------|---------------|
| Lenguaje Backend | Python 3.10+ | Ecosistema robusto para data science y APIs |
| Base de Datos | PostgreSQL (Supabase) | Capacidades relacionales y escalabilidad cloud |
| Visualizaci√≥n | Power BI Desktop/Service | Integraci√≥n empresarial y features interactivos |
| API de Datos | ArXiv API | Acceso abierto a literatura cient√≠fica en CS e IA |
| Orquestaci√≥n ETL | Python Scripts | Control fino y customizaci√≥n |

---

## 3. Arquitectura de Datos

### 3.1 Modelo de Datos Relacional

**Entidades Principales:**
- `papers` - Metadatos de publicaciones
- `authors` - Informaci√≥n de investigadores
- `keywords` - Palabras clave indexadas
- `categories` - Categor√≠as tem√°ticas
- `citations` - Relaciones de citaci√≥n

**Relaciones Clave:**
- `papers_keywords` (N:M) - Permite an√°lisis de co-ocurrencia
- `papers_authors` (N:M) - Autor√≠a m√∫ltiple
- `papers_citations` (N:M) - Grafo de citaciones

---

## 4. Historias de Usuario y Requisitos Funcionales

### EPIC 1: Modelado de Datos

#### US-1.1: Dise√±o del Esquema Relacional
**Como:** Data Engineer  
**Quiero:** Un esquema de base de datos normalizado  
**Para:** Almacenar metadatos complejos sin redundancia

**Criterios de Aceptaci√≥n:**
- [ ] Esquema en 3NF (Tercera Forma Normal)
- [ ] Relaciones N:M correctamente implementadas
- [ ] √çndices en columnas de b√∫squeda frecuente
- [ ] Constraints de integridad referencial

**Tareas T√©cnicas:**
- [ ] TASK-1.1.1: Crear diagrama ER (Entity-Relationship)
- [ ] TASK-1.1.2: Escribir scripts DDL para PostgreSQL
- [ ] TASK-1.1.3: Implementar triggers para auditor√≠a
- [ ] TASK-1.1.4: Documentar diccionario de datos

---

#### US-1.2: Configuraci√≥n de Base de Datos en Supabase
**Como:** DevOps Engineer  
**Quiero:** Una instancia de PostgreSQL configurada  
**Para:** Comenzar la carga de datos

**Criterios de Aceptaci√≥n:**
- [ ] Proyecto creado en Supabase
- [ ] Credenciales almacenadas de forma segura
- [ ] Conexi√≥n verificada desde Python
- [ ] Backups autom√°ticos configurados

**Tareas T√©cnicas:**
- [ ] TASK-1.2.1: Provisionar instancia en Supabase
- [ ] TASK-1.2.2: Configurar variables de entorno (.env)
- [ ] TASK-1.2.3: Crear rol de usuario con permisos apropiados
- [ ] TASK-1.2.4: Verificar conectividad con psycopg2

---

### EPIC 2: Proceso ETL (Extract, Transform, Load)

#### US-2.1: Extracci√≥n de Datos desde ArXiv API
**Como:** Data Engineer  
**Quiero:** Scripts que consuman ArXiv API  
**Para:** Obtener papers de Computational Intelligence

**Criterios de Aceptaci√≥n:**
- [ ] B√∫squeda por categor√≠as relevantes (cs.AI, cs.NE, etc.)
- [ ] Paginaci√≥n implementada para >1000 resultados
- [ ] Manejo de rate limits y errores de red
- [ ] Logs de progreso de extracci√≥n

**Tareas T√©cnicas:**
- [ ] TASK-2.1.1: Implementar cliente ArXiv con `arxiv` library
- [ ] TASK-2.1.2: Crear queries parametrizadas por categor√≠a y fecha
- [ ] TASK-2.1.3: Guardar respuestas raw en formato JSON
- [ ] TASK-2.1.4: Implementar retry logic con backoff exponencial

---

#### US-2.3: Transformaci√≥n de Datos
**Como:** Data Engineer  
**Quiero:** Limpiar y normalizar datos crudos  
**Para:** Asegurar calidad antes de carga

**Criterios de Aceptaci√≥n:**
- [ ] Strings en min√∫sculas y sin caracteres especiales
- [ ] Fechas en formato ISO 8601
- [ ] DOIs validados con regex
- [ ] Duplicados eliminados

**Tareas T√©cnicas:**
- [ ] TASK-2.3.1: Crear pipeline de limpieza con pandas
- [ ] TASK-2.3.2: Implementar validaci√≥n de DOI
- [ ] TASK-2.3.3: Normalizar nombres de autores (ORCID cuando disponible)
- [ ] TASK-2.3.4: Extraer keywords con procesamiento NLP b√°sico

---

#### US-2.4: Carga a PostgreSQL
**Como:** Data Engineer  
**Quiero:** Insertar datos transformados en la base  
**Para:** Habilitar an√°lisis y visualizaciones

**Criterios de Aceptaci√≥n:**
- [ ] Carga incremental (solo nuevos papers)
- [ ] Transacciones ACID para integridad
- [ ] Logging de errores de inserci√≥n
- [ ] Performance >1000 inserts/segundo

**Tareas T√©cnicas:**
- [ ] TASK-2.4.1: Implementar upsert logic con `ON CONFLICT`
- [ ] TASK-2.4.2: Usar `COPY` para carga masiva
- [ ] TASK-2.4.3: Crear funci√≥n de reconciliaci√≥n de IDs
- [ ] TASK-2.4.4: Implementar checkpoint para recuperaci√≥n ante fallos

---

### EPIC 3: Visualizaciones y BI

#### US-3.1: Dashboard de Tendencias de Keywords
**Como:** Investigador  
**Quiero:** Ver palabras clave con mayor crecimiento  
**Para:** Identificar t√≥picos emergentes

**Criterios de Aceptaci√≥n:**
- [ ] Gr√°fico de l√≠nea temporal por keyword
- [ ] Filtros por categor√≠a y a√±o
- [ ] Top 20 keywords con mayor incremento YoY
- [ ] Exportable a PDF

**Tareas T√©cnicas:**
- [ ] TASK-3.1.1: Crear vista SQL para agregaci√≥n de keywords
- [ ] TASK-3.1.2: Dise√±ar visual en Power BI (line chart)
- [ ] TASK-3.1.3: Implementar slicer de a√±o y categor√≠a
- [ ] TASK-3.1.4: Calcular crecimiento interanual con DAX

---

#### US-3.2: Mapa Geogr√°fico de Publicaciones
**Como:** Investigador  
**Quiero:** Visualizar distribuci√≥n geogr√°fica  
**Para:** Entender d√≥nde se concentra la investigaci√≥n

**Criterios de Aceptaci√≥n:**
- [ ] Mapa de burbujas por pa√≠s
- [ ] Tama√±o proporcional a cantidad de papers
- [ ] Tooltip con detalles de instituciones
- [ ] Drill-down por categor√≠a

**Tareas T√©cnicas:**
- [ ] TASK-3.2.1: Enriquecer datos con geolocalizaci√≥n de autores
- [ ] TASK-3.2.2: Crear mapa en Power BI con coordenadas
- [ ] TASK-3.2.3: Configurar tooltips personalizados
- [ ] TASK-3.2.4: Implementar interactividad con otros visuals

---

#### US-3.3: An√°lisis de Impacto vs Longitud
**Como:** Investigador  
**Quiero:** Scatter plot de p√°ginas vs citaciones  
**Para:** Validar si papers m√°s largos tienen mayor impacto

**Criterios de Aceptaci√≥n:**
- [ ] Scatter chart con l√≠nea de tendencia
- [ ] Correlation coefficient calculado
- [ ] Segmentaci√≥n por categor√≠a
- [ ] Outliers destacados

**Tareas T√©cnicas:**
- [ ] TASK-3.3.1: Extraer n√∫mero de p√°ginas de metadata
- [ ] TASK-3.3.2: Crear medida DAX para correlaci√≥n
- [ ] TASK-3.3.3: Dise√±ar scatter plot en Power BI
- [ ] TASK-3.3.4: Implementar conditional formatting

---

### EPIC 4: Escalabilidad y Miner√≠a de Datos (Futuro)

#### US-4.1: An√°lisis de Sentimiento en Abstracts
**Como:** Data Scientist  
**Quiero:** Clasificar abstracts por tono (positivo/neutral/negativo)  
**Para:** Detectar √°reas de investigaci√≥n controversiales

**Criterios de Aceptaci√≥n:**
- [ ] Modelo pre-entrenado de NLP (BERT/RoBERTa)
- [ ] Accuracy >80% en set de validaci√≥n
- [ ] Procesamiento batch de abstracts
- [ ] Resultados almacenados en BD

**Tareas T√©cnicas:**
- [ ] TASK-4.1.1: Seleccionar y descargar modelo de HuggingFace
- [ ] TASK-4.1.2: Crear pipeline de inferencia con transformers
- [ ] TASK-4.1.3: Agregar columna `sentiment_score` a tabla papers
- [ ] TASK-4.1.4: Validar con muestra anotada manualmente

---

#### US-4.2: Topic Modeling con LDA
**Como:** Data Scientist  
**Quiero:** Descubrir t√≥picos latentes en abstracts  
**Para:** Agrupar papers en clusters tem√°ticos

**Criterios de Aceptaci√≥n:**
- [ ] Modelo LDA con 10-20 topics
- [ ] Coherence score >0.5
- [ ] Visualizaci√≥n interactiva (pyLDAvis)
- [ ] Labels interpretables para cada topic

**Tareas T√©cnicas:**
- [ ] TASK-4.2.1: Preprocesar abstracts (tokenizaci√≥n, stopwords)
- [ ] TASK-4.2.2: Entrenar modelo con gensim
- [ ] TASK-4.2.3: Optimizar n√∫mero de topics con grid search
- [ ] TASK-4.2.4: Generar visualizaci√≥n HTML

---

#### US-4.3: Grafo de Colaboraci√≥n entre Autores
**Como:** Investigador  
**Quiero:** Visualizar red de co-autor√≠a  
**Para:** Identificar comunidades y autores clave

**Criterios de Aceptaci√≥n:**
- [ ] Grafo con >500 nodos (autores)
- [ ] Edges representan co-autor√≠a
- [ ] M√©tricas de centralidad calculadas
- [ ] Visualizaci√≥n en Gephi o Networkx

**Tareas T√©cnicas:**
- [ ] TASK-4.3.1: Construir matriz de adyacencia desde BD
- [ ] TASK-4.3.2: Crear grafo con networkx
- [ ] TASK-4.3.3: Calcular betweenness centrality y PageRank
- [ ] TASK-4.3.4: Exportar a formato GEXF para Gephi

---

#### US-4.4: Predicci√≥n de Hot Topics
**Como:** Investigador  
**Quiero:** Modelo predictivo de tendencias  
**Para:** Anticipar pr√≥ximas √°reas de investigaci√≥n

**Criterios de Aceptaci√≥n:**
- [ ] Serie temporal de keywords por a√±o
- [ ] Modelo ARIMA o Prophet entrenado
- [ ] Predicci√≥n para pr√≥ximos 2 a√±os
- [ ] Intervalo de confianza del 95%

**Tareas T√©cnicas:**
- [ ] TASK-4.4.1: Agregar series temporales por keyword
- [ ] TASK-4.4.2: Probar ARIMA vs Prophet vs LSTM
- [ ] TASK-4.4.3: Validar con hold-out temporal
- [ ] TASK-4.4.4: Crear dashboard de predicciones

---

## 5. Especificaciones T√©cnicas

### 5.1 Estructura del Repositorio

```
analisis_bibliometrico/
‚îú‚îÄ‚îÄ data/                       # Datos y esquemas
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # JSON responses de APIs
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Datos limpios
‚îÇ   ‚îî‚îÄ‚îÄ sql/                    # Scripts DDL y DML
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks experimentales
‚îÇ   ‚îú‚îÄ‚îÄ etl_api_arxiv.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb
‚îú‚îÄ‚îÄ src/                        # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ extractors/             # M√≥dulos de API clients
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ arxiv_client.py
‚îÇ   ‚îú‚îÄ‚îÄ transformers/           # Pipeline de limpieza
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py
‚îÇ   ‚îú‚îÄ‚îÄ loaders/                # Carga a BD
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ postgres_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # ORM y schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database_models.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Utilidades
‚îÇ       ‚îú‚îÄ‚îÄ logger.py
‚îÇ       ‚îî‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ powerbi/                    # Archivos Power BI
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.pbix
‚îú‚îÄ‚îÄ tests/                      # Tests unitarios
‚îÇ   ‚îî‚îÄ‚îÄ test_extractors.py
‚îú‚îÄ‚îÄ .env.example                # Template de variables
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python
‚îú‚îÄ‚îÄ PRD.md                      # Este documento
‚îú‚îÄ‚îÄ SKILLS.md                   # Skills para agentes GenAI
‚îú‚îÄ‚îÄ SDD.md                      # Software Design Document
‚îî‚îÄ‚îÄ README.md                   # Documentaci√≥n principal
```

### 5.2 Dependencias Principales

```txt
# Core
python>=3.10
pandas>=2.0.0
numpy>=1.24.0

# Database
psycopg2-binary>=2.9.0
sqlalchemy>=2.0.0

# API Clients
requests>=2.31.0
arxiv>=2.0.0

# NLP (Futuro)
transformers>=4.30.0
gensim>=4.3.0
spacy>=3.5.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
networkx>=3.1

# Utils
python-dotenv>=1.0.0
tqdm>=4.65.0
```

### 5.3 Variables de Entorno

```bash
# Database
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=your_anon_key
DB_HOST=db.xxxxx.supabase.co
DB_PORT=5432
DB_NAME=postgres
DB_USER=postgres
DB_PASSWORD=your_password

# Config
LOG_LEVEL=INFO
BATCH_SIZE=1000
```

---

## 6. Riesgos y Mitigaciones

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Rate limiting de ArXiv API | Media | Medio | Implementar caching y distribuci√≥n temporal |
| Cambios en esquema de ArXiv API | Baja | Medio | Versionar API client y tests de integraci√≥n |
| Calidad de datos inconsistente | Alta | Alto | Pipeline robusto de validaci√≥n |
| Sobrecarga de BD con papers duplicados | Media | Medio | Constraints UNIQUE y deduplicaci√≥n |

---

## 7. Plan de Entrega

### Fase 1: Fundamentos (Semanas 1-2)
- ‚úÖ Modelado de datos completo
- ‚úÖ Base de datos configurada
- ‚úÖ Scripts DDL ejecutados

### Fase 2: ETL Core (Semanas 3-5)
- üîÑ Extractor de ArXiv
- üîÑ Pipeline de transformaci√≥n
- ‚¨ú Carga automatizada a PostgreSQL

### Fase 3: Visualizaci√≥n (Semanas 6-7)
- ‚¨ú Dashboard b√°sico en Power BI
- ‚¨ú Tres visualizaciones principales

### Fase 4: Miner√≠a Avanzada (Futuro)
- ‚¨ú NLP y topic modeling
- ‚¨ú An√°lisis de redes
- ‚¨ú Modelos predictivos

---

## 8. Criterios de Calidad

### 8.1 Cobertura de Tests
- M√≠nimo 80% de code coverage
- Tests unitarios para cada extractor
- Tests de integraci√≥n para ETL completo

### 8.2 Performance
- ETL procesa >5000 papers/hora
- Queries del dashboard <500ms
- Uptime de BD >99.5%

### 8.3 Documentaci√≥n
- Docstrings en formato Google Style
- README actualizado con ejemplos
- Diagramas de arquitectura

---

## 9. Glosario

- **DOI**: Digital Object Identifier, identificador √∫nico de papers
- **ETL**: Extract, Transform, Load
- **LDA**: Latent Dirichlet Allocation, algoritmo de topic modeling
- **N:M**: Relaci√≥n muchos a muchos
- **ORCID**: Identificador √∫nico de investigadores
- **YoY**: Year over Year, crecimiento interanual

---

## 10. Aprobaciones

| Rol | Nombre | Fecha | Firma |
|-----|--------|-------|-------|
| Product Owner | Lucas Gabirondo | 2026-01-26 | ‚úì |
| Tech Lead | - | - | - |

---

**√öltima actualizaci√≥n:** 6 de Febrero, 2026  
**Pr√≥xima revisi√≥n:** 28 de Febrero, 2026
